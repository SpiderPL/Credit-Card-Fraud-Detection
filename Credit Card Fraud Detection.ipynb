{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected during a research collaboration of Worldline and the Machine Learning Group. A model trained on this data that is seen as a good fit could then be used to make certain recognize fraudulent credit card transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit card fraud detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit√© Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, auc, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable \n",
    "SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping\n",
    "new_folder = \"Data\"\n",
    "zip_file = os.path.join(new_folder, \"creditcardfraud.zip\")\n",
    "new_files_location = os.path.join(os.getcwd(), new_folder)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(new_files_location)\n",
    "except:\n",
    "    print(\"Error during unzipping\")\n",
    "\n",
    "del zip_file, new_files_location\n",
    "\n",
    "raw_data_file = os.path.join(new_folder, \"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "raw_data = pd.read_csv(raw_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shape\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "raw_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the missing values for each column\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look inside data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check distribution of class column\n",
    "count_classes = pd.value_counts(raw_data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us check in the number of Percentage\n",
    "dataframe_class = raw_data['Class'].value_counts().to_frame().reset_index()\n",
    "dataframe_class['percent'] = dataframe_class[\"Class\"].apply(lambda x : round(100*float(x) / len(raw_data), 2))\n",
    "dataframe_class = dataframe_class.rename(columns = {\"index\" : \"Target\", \"Class\" : \"Count\"})\n",
    "dataframe_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is imbalanced, there is only 0.17 % are the fraud transcation while 99.83 are valid transcation.\n",
    "Collect more data not applicable in this case. So now we have to do resampling or oversampling (include generate synthetic samples) of this data. In this case it is not possible collect more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover we have to change metrics, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include: Confusion Matrix, Precision, Recall or F1. I goint to use confusion matrix and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check distribution of transactions in time.\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
    "\n",
    "bins = 100\n",
    "\n",
    "ax1.hist(raw_data.Time[raw_data.Class == 1], bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(raw_data.Time[raw_data.Class == 0], bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotHistogram(df,norm):\n",
    "    bins = np.arange(df['hour'].min(),df['hour'].max()+2)\n",
    "    plt.figure(figsize=(15,4))\n",
    "    sns.distplot(df[df['Class']==0.0]['hour'],\n",
    "                 norm_hist=norm,\n",
    "                 bins=bins,\n",
    "                 kde=False,\n",
    "                 color='b',\n",
    "                 hist_kws={'alpha':.5},\n",
    "                 label='Legit')\n",
    "    sns.distplot(df[df['Class']==1.0]['hour'],\n",
    "                 norm_hist=norm,\n",
    "                 bins=bins,\n",
    "                 kde=False,\n",
    "                 color='r',\n",
    "                 label='Fraud',\n",
    "                 hist_kws={'alpha':.5})\n",
    "    plt.xticks(range(0,24))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['hour'] = raw_data['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normalized histogram of Legit/Fraud over hour of the day')\n",
    "PlotHistogram(raw_data,True)\n",
    "print('Counts histogram of Legit/Fraud over hour of the day')\n",
    "PlotHistogram(raw_data,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can barely see the Fraud cases since there are so little of them. Hour of the day seems to have some impact on the number of Fraud cases. I'll be sure to to add the 'hour' dimension to visualizations later to further investigate its impact.\n",
    "\n",
    "Before we train our classifers, we need to normalize the Amount since it's on a totally different scale. The distributions are also highly skewed with a lot of statistical outliers. All Fraud cases are in the low dollar values i.e. Amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Check distribution of amount in time.\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n",
    "\n",
    "ax1.scatter(raw_data.Time[raw_data.Class == 1], raw_data.Amount[raw_data.Class == 1])\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.scatter(raw_data.Time[raw_data.Class == 0], raw_data.Amount[raw_data.Class == 0])\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The 'Time' feature looks pretty similar across both types of transactions. You could argue that fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. This could make it easier to detect a fraudulent transaction during at an 'off-peak' time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transaction amount differs between the two types.\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(12,8))\n",
    "\n",
    "bins = 8\n",
    "\n",
    "ax1.hist(raw_data.Amount[raw_data.Class == 1], bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(raw_data.Amount[raw_data.Class == 0], bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Amount (in dolars)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most transactions are small amounts, less than 100 dollars.\n",
    "Fraudulent transactions have a maximum value far less than normal transactions, \n",
    "2,125.87 dollars vs 25,691.16 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Fraud\")\n",
    "print (raw_data.Amount[raw_data.Class == 1].describe())\n",
    "print ()\n",
    "print (\"Normal\")\n",
    "print (raw_data.Amount[raw_data.Class == 0].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean amount fraud transactions is twice less than normal. 122 dollars vs 250 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the anonymized features.\n",
    "v_features = raw_data.ix[:,1:29].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,28*4))\n",
    "gs = gridspec.GridSpec(28, 1)\n",
    "for i, cn in enumerate(raw_data[v_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(raw_data[cn][raw_data.Class == 1], bins=50)\n",
    "    sns.distplot(raw_data[cn][raw_data.Class == 0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all of the features that have very similar distributions between the two types of transactions.\n",
    "df_drop = raw_data.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize amount of transaction\n",
    "raw_data['norm_Amount'] = StandardScaler().fit_transform(pd.DataFrame(raw_data['Amount']))\n",
    "raw_data = raw_data.drop(['Amount'], axis=1)\n",
    "\n",
    "df_drop['norm_Amount'] = StandardScaler().fit_transform(pd.DataFrame(df_drop['Amount']))\n",
    "df_drop = df_drop.drop(['Amount'], axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to X and y\n",
    "X = raw_data.drop('Class', 1)\n",
    "y = raw_data.Class\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling by SMOTE methods\n",
    "smote = SMOTE(random_state=SEED)\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimum_C(model_X, X_train_smote, y_train_smote):\n",
    "    #Cross validate with C parameter \n",
    "    parameters = {\n",
    "    'C': np.linspace(1, 10, 2)\n",
    "             }\n",
    "    clf = GridSearchCV(model_X, parameters, cv=5, verbose=5, n_jobs=3, scoring = 'recall')\n",
    "    clf.fit(X_train_smote, y_train_smote.ravel())\n",
    "    return (clf.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_m(y_test, y_pre):\n",
    "    #Plot non-normalized confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pre)\n",
    "    class_names = [0,1]\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plot_confusion_matrix(cnf_matrix , classes=class_names, title='Confusion matrix')\n",
    "    plt.show()\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_c(y_test, y_pred_sample_score):\n",
    "    #Plot ROC curve\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    plt.title('Roc curve')\n",
    "    plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(cnf_matrix,y_test, y_pre):\n",
    "    #Write recall metric, confusion matrix and bigger classification report\n",
    "    print(\"Recall metric in the testing dataset: {}%\".format(round(100*cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])),4))\n",
    "    print(confusion_matrix(y_test, y_pre))\n",
    "    print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(y_test, y_pre, y_pred_sample_score):\n",
    "    #Show all results\n",
    "    cnf_matrix = plot_confusion_m(y_test, y_pre)\n",
    "    plot_roc_c(y_test, y_pred_sample_score)\n",
    "    write_results(cnf_matrix,y_test, y_pre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_without_auc(y_test, y_pre):\n",
    "    #Show all results without roc curve\n",
    "    cnf_matrix = plot_confusion_m(y_test, y_pre)\n",
    "    write_resoults(cnf_matrix,y_test, y_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model Logistic Regression\n",
    "model1 = LogisticRegression(penalty='l1', verbose=5, solver='liblinear')\n",
    "#Find optimum C\n",
    "opt_C = optimum_C(model1, X_train_smote, y_train_smote)\n",
    "print('optimum C is ', opt_C)\n",
    "#Prepare model with the best parameter C\n",
    "lr1 = LogisticRegression(C=opt_C,penalty='l1', verbose=5, solver='liblinear')\n",
    "#Learn model\n",
    "lr1.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr1.predict(X_test)\n",
    "tmp = lr1.fit(X_train_smote, y_train_smote.ravel())\n",
    "y_pred_sample_score = tmp.decision_function(X_test)\n",
    "#Print results\n",
    "show_results(y_test, y_pre, y_pred_sample_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model GaussianNB\n",
    "lr1 = GaussianNB()\n",
    "#Learn model\n",
    "lr1.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr1.predict(X_test)\n",
    "tmp = lr1.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameter n_neighbors for K-NN\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "acc = 0\n",
    "\n",
    "#Train some models with using recall score\n",
    "for i in range(1,15):\n",
    "    model_lr = KNeighborsClassifier(n_neighbors=i,metric = 'minkowski', p = 2)\n",
    "    model_lr.fit(X_train_smote, y_train_smote.ravel())\n",
    "    pred_al = model_lr.predict(X_train_smote)\n",
    "    pred_lr = model_lr.predict(X_test)\n",
    "    train_accuracy.append(recall_score(y_test, pred_lr))\n",
    "    test_accuracy.append(recall_score(y_train_smote.ravel(), pred_al))\n",
    "    acc=0  \n",
    "print(' Sample number: ',np.argmax(train_accuracy)+1, ' Train error equals:', train_accuracy[np.argmax(train_accuracy)] )\n",
    "print(' Sample number: ',np.argmax(test_accuracy)+1, ' Validation error equals:', test_accuracy[np.argmax(test_accuracy)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot recall for K parameter for K Neighboors Classifier \n",
    "fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "plt.title('K for KNeighborsClassifier')\n",
    "plt.plot(train_accuracy, color='r')\n",
    "#Print recall for train data set\n",
    "ax1.set_ylabel('train_recall',color='r')\n",
    "plt.legend(['train_recall'],loc=(0.01,0.95))\n",
    "ax2 = ax1.twinx()\n",
    "#Print recall for test data set\n",
    "plt.plot(test_accuracy,color='b')\n",
    "ax2.set_ylabel('test_recall',color='b')\n",
    "plt.legend(['test_recall'],loc=(0.01,0.90))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose KNN model\n",
    "model3 = KNeighborsClassifier(n_neighbors = 2, metric = 'minkowski', p = 2)\n",
    "#Learn model\n",
    "model3.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = model3.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose DecisionTreeClassifier model\n",
    "model4 = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model4.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model4.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose RandomForestClassifier model\n",
    "model5 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model5.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model5.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$how features importances in Random Forest Classifier\n",
    "importances = pd.Series(model5.feature_importances_, index=X.columns)\n",
    "importances.plot(kind='barh', figsize=(12,8),title='Features importances in Random Forest Classifier ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 10 the most important features in Random Forest Classifier\n",
    "sorted_importances_model5 = pd.Series(model5.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print (sorted_importances_model5[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances_model5.index.values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10 best features from Random Forest Classifier on raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = sorted_importances_model5.index.values[:10]\n",
    "best_features = list(best_features)\n",
    "X.drop(X.columns.difference(best_features), 1, inplace=True)\n",
    "X = raw_data.loc[:, raw_data.columns.intersection(a)]\n",
    "y = raw_data.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling by SMOTE methods\n",
    "smote = SMOTE(random_state=SEED)\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model Logistic Regression\n",
    "model6 = LogisticRegression(penalty='l1', verbose=5, solver='liblinear')\n",
    "#Find optimum C\n",
    "opt_C = optimum_C(model6, X_train_smote, y_train_smote)\n",
    "print('optimum C is ', opt_C)\n",
    "#Prepare model with the best parameter C\n",
    "lr6 = LogisticRegression(C=opt_C,penalty='l1', verbose=5, solver='liblinear')\n",
    "#Learn model\n",
    "lr6.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr6.predict(X_test)\n",
    "tmp = lr6.fit(X_train_smote, y_train_smote.ravel())\n",
    "y_pred_sample_score = tmp.decision_function(X_test)\n",
    "#Print results\n",
    "show_results(y_test, y_pre, y_pred_sample_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model GaussianNB\n",
    "lr7 = GaussianNB()\n",
    "#Learn model\n",
    "lr7.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr7.predict(X_test)\n",
    "tmp = lr7.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameter n_neighbors for K-NN\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "acc = 0\n",
    "\n",
    "#Train some models with using recall score\n",
    "for i in range(1,15):\n",
    "    model_lr = KNeighborsClassifier(n_neighbors=i,metric = 'minkowski', p = 2)\n",
    "    model_lr.fit(X_train_smote, y_train_smote.ravel())\n",
    "    pred_al = model_lr.predict(X_train_smote)\n",
    "    pred_lr = model_lr.predict(X_test)\n",
    "    train_accuracy.append(recall_score(y_test, pred_lr))\n",
    "    test_accuracy.append(recall_score(y_train_smote.ravel(), pred_al))\n",
    "    acc=0  \n",
    "print(' Sample number: ',np.argmax(train_accuracy)+1, ' Train error equals:', train_accuracy[np.argmax(train_accuracy)] )\n",
    "print(' Sample number: ',np.argmax(test_accuracy)+1, ' Validation error equals:', test_accuracy[np.argmax(test_accuracy)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot recall for K parameter for K Neighboors Classifier \n",
    "fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "plt.title('K for KNeighborsClassifier')\n",
    "plt.plot(train_accuracy, color='r')\n",
    "#Print recall for train data set\n",
    "ax1.set_ylabel('train_recall',color='r')\n",
    "plt.legend(['train_recall'],loc=(0.01,0.95))\n",
    "ax2 = ax1.twinx()\n",
    "#Print recall for test data set\n",
    "plt.plot(test_accuracy,color='b')\n",
    "ax2.set_ylabel('test_recall',color='b')\n",
    "plt.legend(['test_recall'],loc=(0.01,0.90))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose KNN model\n",
    "model7 = KNeighborsClassifier(n_neighbors = 2, metric = 'minkowski', p = 2)\n",
    "#Learn model\n",
    "model7.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = model7.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose DecisionTreeClassifier model\n",
    "model8 = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model8.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model8.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose RandomForestClassifier model\n",
    "model9 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model9.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model9.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use data with droped all of the features that have very similar distributions between the two types of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to X and y\n",
    "X = df_drop.drop('Class', 1)\n",
    "y = df_drop.Class\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling by SMOTE methods\n",
    "smote = SMOTE(random_state=SEED)\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model Logistic Regression\n",
    "model10 = LogisticRegression(penalty='l1', verbose=5, solver='liblinear')\n",
    "#Find optimum C\n",
    "opt_C = optimum_C(model10, X_train_smote, y_train_smote)\n",
    "print('optimum C is ', opt_C)\n",
    "#Prepare model with the best parameter C\n",
    "lr10 = LogisticRegression(C=opt_C,penalty='l1', verbose=5, solver='liblinear')\n",
    "#Learn model\n",
    "lr10.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr10.predict(X_test)\n",
    "tmp = lr10.fit(X_train_smote, y_train_smote.ravel())\n",
    "y_pred_sample_score = tmp.decision_function(X_test)\n",
    "#Print results\n",
    "show_results(y_test, y_pre, y_pred_sample_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose model GaussianNB\n",
    "lr11 = GaussianNB()\n",
    "#Learn model\n",
    "lr11.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = lr11.predict(X_test)\n",
    "tmp = lr11.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameter n_neighbors for K-NN\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "acc = 0\n",
    "\n",
    "#Train some models with using recall score\n",
    "for i in range(1,15):\n",
    "    model_lr = KNeighborsClassifier(n_neighbors=i,metric = 'minkowski', p = 2)\n",
    "    model_lr.fit(X_train_smote, y_train_smote.ravel())\n",
    "    pred_al = model_lr.predict(X_train_smote)\n",
    "    pred_lr = model_lr.predict(X_test)\n",
    "    train_accuracy.append(recall_score(y_test, pred_lr))\n",
    "    test_accuracy.append(recall_score(y_train_smote.ravel(), pred_al))\n",
    "    acc=0  \n",
    "print(' Sample number: ',np.argmax(train_accuracy)+1, ' Train error equals:', train_accuracy[np.argmax(train_accuracy)] )\n",
    "print(' Sample number: ',np.argmax(test_accuracy)+1, ' Validation error equals:', test_accuracy[np.argmax(test_accuracy)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot recall for K parameter for K Neighboors Classifier \n",
    "fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "plt.title('K for KNeighborsClassifier')\n",
    "plt.plot(train_accuracy, color='r')\n",
    "#Print recall for train data set\n",
    "ax1.set_ylabel('train_recall',color='r')\n",
    "plt.legend(['train_recall'],loc=(0.01,0.95))\n",
    "ax2 = ax1.twinx()\n",
    "#Print recall for test data set\n",
    "plt.plot(test_accuracy,color='b')\n",
    "ax2.set_ylabel('test_recall',color='b')\n",
    "plt.legend(['test_recall'],loc=(0.01,0.90))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose KNN model\n",
    "model12 = KNeighborsClassifier(n_neighbors = 2, metric = 'minkowski', p = 2)\n",
    "#Learn model\n",
    "model12.fit(X_train_smote, y_train_smote.ravel())\n",
    "#Predict label in test set\n",
    "y_pre = model12.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose DecisionTreeClassifier model\n",
    "model13 = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model13.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model13.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose RandomForestClassifier model\n",
    "model14 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 1)\n",
    "#Learn model\n",
    "model14.fit(X_train, y_train)\n",
    "#Predict label in test set\n",
    "y_pre = model14.predict(X_test)\n",
    "#Print results\n",
    "show_results_without_auc(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
